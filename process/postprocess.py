import re
import os
import yaml
from typing import Dict, List
import nltk
from nltk import tokenize

class PostProcess:
    def __init__(self, title_case = True, ending_exclusions = '.!#', exclude_exclamation = True,
    exclude_phone_number = True, exclude_domain = True, 
    match_capitalization_with_input = False) -> None:
        self.title_case = title_case
        self.ending_exclusions = ending_exclusions
        self.exclude_exclamation = exclude_exclamation
        # TODO: implement below
        self.exclude_phone_number = exclude_phone_number
        self.exclude_domain = exclude_domain
        self.match_capitalization_with_input = match_capitalization_with_input

    def run(self, gpt3_ouput_str: str=str(), input_dict: Dict=dict(), preserve_capital_keys=['bu_name', 'interest_keyword', 'brand_name'], reference_input = '', separator = ''):
        '''
        Main Function for processing the generations to business usecase
        
        Parameters:
            gpt3_ouput_str (str): Input string generated by gpt3
            input_dict (dict): Helper dictionary for post processing
            preserve_capital_keys (List): List of input_dict keys to preserve the case of generations

        Returns:
            processed generation (str):
        '''
        
        ## Compliance
        brand_id = input_dict.get('brand_id', '')
        # Filter by compliance requirements for account name
        compliance_file_path = 'ds/process/brand_specific/' + str(brand_id) + '.yaml'
        if os.path.exists(compliance_file_path):
            with open(compliance_file_path, "r") as f:
                data = yaml.safe_load(f)
          
            replacements_mapping = data.get('replacements')
            # To ensure that we only do the replacements if the yaml file actually has that field otherwise we would get an empty generations list
            if replacements_mapping:
                replacements_mapping = {key.lower(): value for key, value in replacements_mapping.items()}
                pattern = re.compile(r'\b(' + '|'.join([re.escape(key) for key in replacements_mapping.keys()]) + r')\b', re.IGNORECASE)
                gpt3_ouput_str = pattern.sub(
                    lambda match: replacements_mapping[match.group(0).lower()].lower() if match.group(0).islower() else replacements_mapping[match.group(0).lower()].upper() if match.group(0).isupper() else replacements_mapping[match.group(0).lower()].title() if match.group(0).istitle() else replacements_mapping[match.group(0).lower()], gpt3_ouput_str)
    
        remove_junk_str = self.remove_junk_characters(gpt3_ouput_str)
        fix_article = self.fix_article(remove_junk_str)
        fix_punctuation = self.process_puntuation(fix_article)
        raw_material = ' '.join([value for value in input_dict.values() if type(value) == type('asdf')])
        new_text = self.fix_incorrect_offer(raw_material, fix_punctuation)
        new_text = self.fix_unusual_capitalization(raw_material, new_text)
        new_text = self.remove_unclosed_quote(new_text)

        for preserve_key in preserve_capital_keys: #['bu_name', 'interest_keyword']:
            if preserve_key in input_dict:
                if preserve_key in ['brand_name', 'bu_name']:
                    new_text = self.fix_brand_name(input_dict[preserve_key], new_text)
                else:
                    new_text = self.fix_target_word(input_dict[preserve_key], new_text)

        fix_capitalisation_str = self.process_capitalisation(new_text, reference_input, separator)
        new_text = fix_capitalisation_str

        return new_text.strip()

    def remove_junk_characters(self, text):
        new_text = text.strip()
        for exclusions in self.ending_exclusions:
            new_text = new_text.strip(exclusions)
        new_text =  re.sub(r'[<>"]', '', new_text)
        if self.exclude_exclamation:
            new_text =  re.sub('!', '.', new_text)
        return new_text
    
    def process_capitalisation(self, text, reference_text, separator):
        "ByPi's --> BYJU's " 
        if text:
            if self.match_capitalization_with_input:
                
                if not reference_text:
                    return text.title()
                
                if not separator:
                    reference_text = reference_text
                    separator = ' '
                else:
                    reference_text = reference_text.replace(separator, '')
                
                if reference_text.isupper():
                    new_text = f'{separator}'.join([sen.upper().strip() for sen in text.split(separator)])
                elif reference_text.islower():
                    new_text = f'{separator}'.join([sen.lower().strip() for sen in text.split(separator)])
                elif reference_text.istitle():
                    new_text = f'{separator}'.join([sen.title().strip() for sen in text.split(separator)])
                elif reference_text[0].isupper() and reference_text[1:].islower():
                    new_text = text[0].upper() + f'{separator}'.join([sen.lower().strip() for sen in text[1:].split(separator)])
                else:
                    new_text = text.split()
                    new_text = [word.title() if word.islower() else word for word in new_text]
                    new_text = ' '.join(new_text)
                return new_text.strip()
            
            else:
                if self.title_case:
                    # title case
                    new_text = text.split()
                    new_text = [word.title() if word.islower() else word for word in new_text]
                    new_text = ' '.join(new_text)
                    return new_text.strip()
                
                else:
                    # sentence case
                    # text = text.lower()
                    sentence_case_text = ''
                    for line in text.split('\n'):
                        # sentences = tokenize.sent_tokenize(line)
                        sentences = english_tokenizer.tokenize(line)
                        sentences = [el[0].upper() + el[1:] for el in sentences]
                        sentences = ' '.join(sentences)
                        sentence_case_text += sentences+'\n'

                    return sentence_case_text.strip()
        else:
            return text


    def fix_brand_name(self, actual_brand_name, text):
        brand_regex = '\s*'.join(actual_brand_name)
        new_text = re.sub(brand_regex, actual_brand_name, text, flags=re.IGNORECASE)
        return new_text.strip()
    

    def process_puntuation(self, text):
        # remove consecutive punctuations
        new_text = re.sub(r'([.?!<>,@#$%&*();:+=\-_])([.?!<>,@#$%&*();:+=\-_])+', r'\1', text)
        #Remove space before punctations
        new_text = re.sub(r' ([,.:;!?])', r'\1', new_text)
        # add space after punctuations
        new_text = re.sub(r'([,.:;&!?])(?!\d+)', r'\1 ', new_text)
        
        # TODO: remove consicutive white spaces
        # remove consicutive white spaces
        new_text = re.sub(r' +', ' ', new_text)

        # Fix: Domain extensions .com .in etc.
        domain_format_regex = r"(?:[A-Za-z0-9-]{1,63}\.)+[a-zA-Z]{2,6}"
        domain_matches = re.findall(domain_format_regex, text)

        for domain_match in domain_matches:
            new_text = re.sub(domain_match.replace('.', '. '), domain_match , new_text)
        
        return new_text.strip()


    def remove_unclosed_quote(self, text):
        # Checking count of double-quotes and removing any unclosed ones if present
        if text.count('"')%2 == 0:
            return text
        # Remove unclosed quote marks
        text = re.sub(r'"([^"]*)$', r'\1', text)
        return text
    

    def fix_article(self, text):
        """Fixes a / an articles based on vowels

        Params:
        text -- input text

        """
        false_pos = ['unique', 'europe', 'url', 'unicorn', 'eulogy', 'used', 'one', 'once', 'ones']
        false_neg = ['hour', 'honor', 'heir', 'hourglass']
        text = ' '+text
        text = re.sub(r" an ([^aeiou])", r" a \1", text, flags=re.IGNORECASE)
        text = re.sub(r" a ([aeiou])", r" an \1", text, flags=re.IGNORECASE)

        for el in false_pos:
            # text = text.replace(' an ' + el, ' a ' + el)
            text = re.sub(r" an " + el, r" a " + el, text, flags=re.IGNORECASE)

        for el in false_neg:
            # text = text.replace(' a ' + el, ' an ' + el)
            text = re.sub(r" a " + el, r" an " + el, text, flags=re.IGNORECASE)

        return text.strip()

    def fix_target_word(self, target_word, text):
        # text = ' '+text
        # target_word = ' '+target_word

        target_word = re.sub('\(|\)|\[|\]', '', target_word)
        # remove chars that causes issues in following regex

        target_word_list = target_word.split()
        new_text = text
        for target_word in target_word_list:
            new_text = re.sub('(?:(?<=\s)|(?<=^))' + target_word + '(?:(?=\W)|(?=$))', target_word, new_text, flags=re.IGNORECASE)
            ## Regex Explanation
            # (?<=\s) = look behind for white space characters
            # (?<=^) = look behind for start of string
            # (?=\W) = look ahead for non alphanumeric characters
            # (?=$) = look ahead for end of string
            # the above regex looks for target_word without any alphanumeric prefix or suffix

        return new_text.strip()
    
    
    def fix_incorrect_offer(self, orig_text, gen_text):
        
        orig_per_off = re.findall(r'[0-9][0-9. ]+%', orig_text)
        gen_per_off = re.findall(r'[0-9][0-9. ]+%', gen_text)

        orig_usd_off = re.findall(r'\$[0-9., ]+', orig_text)
        gen_usd_off = re.findall(r'\$[0-9., ]+', gen_text)

        if gen_per_off and orig_per_off:
            for old_off in gen_per_off:
                tar_off = orig_per_off[0] if orig_per_off else orig_usd_off[0] if orig_usd_off else 'XOFF'    
                gen_text = gen_text.replace(old_off.strip(), tar_off.strip())


        if gen_usd_off and orig_usd_off:
            for old_off in gen_usd_off:
                tar_off = orig_per_off[0] if orig_per_off else orig_usd_off[0] if orig_usd_off else 'XOFF'
                gen_text = gen_text.replace(old_off.strip(), tar_off.strip())

        return gen_text

    def fix_unusual_capitalization(self, orig_text, gen_text):
        # find words with unusual cap in orig_text and replace in gen_text
        gloss = set(re.findall(r'\S+[A-Z-]\S*', orig_text))

        # remove special characters from start and end
        gloss = [re.sub(r'^\W?', '', el) for el in gloss]
        gloss = [re.sub(r'\W?$', '', el) for el in gloss]

        text = gen_text
        for word in gloss:
            text = self.fix_target_word(word, text)

        return text.strip()
    

    ##TODO Fixing Brand and Offer


if __name__ == '__main__':
    # test capitalisation
    test_set = [
        'âœ… No office visits or pharmacy trips required',
        'ðŸ” Fast and Secure Online Treatment',
        '3. Ingredients: clinically proven at more personalized dosages',
        "'âœ… No office visits or pharmacy trips required",
        "Open a Senior Citizen Account ðŸ‘´ðŸ»",
        "DOCTOR are good",
        "quick and fast",
        "doctor ",
        "mental health provider in days",
        "free medication adjustments",
        "offer quick resolve"
 ]
    idi = {
        "bu_detail" : '''HIMSS is a healthcare research and advisory firm that specializes in guidance and market intelligence. It has various models that include outpatient electronic medical record adoption, infrastructure adoption, analytics maturity adoption and certified consultant program models.''',
        "refrence_plain_primary_text": '''Join healthcare professionals, technology providers and government representatives as we look at the transformation of healthcare across the region.''',
        "reference_headline": 'Manifest transformative health tech at HIMSS23.',
        "reference_description": '',
        "brand_name" : 'hims',
        'bu_name': 'hims',
        'interest_keyword': 'Mental Health',
        "brand_id" : "f226bd37-db7c-4908-b992-907ff441bcb7",
        # "brand_id" : "a31704be-7b6d-442a-bf94-bf2bc7084263",
        "n_generations" : 15
    }
    post_process_obj = PostProcess(title_case=False, exclude_exclamation=False)
    for generation in test_set:
        print(post_process_obj.run(generation, input_dict=idi, preserve_capital_keys=['interest_keyword', 'bu_name']))
